"""
cp_api/services/csv_importer.py

æ–‡åŒ–è²¡CSVã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚µãƒ¼ãƒ“ã‚¹

æ©Ÿèƒ½:
- è‡ªæ²»ä½“æ¨™æº–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå½¢å¼ã®CSVè§£æ
- ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆåº§æ¨™ã€å¿…é ˆé …ç›®ã€é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼‰
- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®ä¸€æ‹¬ç™»éŒ²
- ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•åˆ¤å®šï¼ˆUTF-8, UTF-16, Shift-JISç­‰ï¼‰
"""

import io
import csv
import uuid
import logging
from dataclasses import dataclass, field
from typing import List, Optional, Dict, Any, Tuple
from enum import Enum

from django.db import transaction
from django.contrib.gis.geos import Point
from django.core.cache import cache

logger = logging.getLogger(__name__)


class ImportStatus(Enum):
    """ã‚¤ãƒ³ãƒãƒ¼ãƒˆè¡Œã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹"""
    VALID = "valid"           # æœ‰åŠ¹ï¼ˆã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯èƒ½ï¼‰
    ERROR = "error"           # ã‚¨ãƒ©ãƒ¼ï¼ˆã‚¤ãƒ³ãƒãƒ¼ãƒˆä¸å¯ï¼‰
    DUPLICATE = "duplicate"   # é‡è¤‡ï¼ˆæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨é‡è¤‡ï¼‰
    WARNING = "warning"       # è­¦å‘Šï¼ˆã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯èƒ½ã ãŒæ³¨æ„ãŒå¿…è¦ï¼‰


@dataclass
class ImportRow:
    """ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯¾è±¡ã®1è¡Œã‚’è¡¨ã™ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    row_number: int                      # è¡Œç•ªå·ï¼ˆ1å§‹ã¾ã‚Šã€ãƒ˜ãƒƒãƒ€ãƒ¼é™¤ãï¼‰
    status: ImportStatus = ImportStatus.VALID
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    raw_data: Dict[str, Any] = field(default_factory=dict)
    
    # ãƒãƒƒãƒ”ãƒ³ã‚°å¾Œã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
    name: Optional[str] = None
    name_kana: Optional[str] = None
    name_en: Optional[str] = None
    category: Optional[str] = None
    type: Optional[str] = None
    place_name: Optional[str] = None
    address: Optional[str] = None
    latitude: Optional[float] = None
    longitude: Optional[float] = None
    url: Optional[str] = None
    note: Optional[str] = None
    
    # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨
    duplicate_id: Optional[int] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """è¾æ›¸å½¢å¼ã«å¤‰æ›"""
        return {
            'row_number': self.row_number,
            'status': self.status.value,
            'errors': self.errors,
            'warnings': self.warnings,
            'name': self.name,
            'name_kana': self.name_kana,
            'name_en': self.name_en,
            'category': self.category,
            'type': self.type,
            'place_name': self.place_name,
            'address': self.address,
            'latitude': self.latitude,
            'longitude': self.longitude,
            'url': self.url,
            'note': self.note,
            'duplicate_id': self.duplicate_id,
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'ImportRow':
        """è¾æ›¸ã‹ã‚‰ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”Ÿæˆ"""
        row = cls(row_number=data['row_number'])
        row.status = ImportStatus(data['status'])
        row.errors = data.get('errors', [])
        row.warnings = data.get('warnings', [])
        row.name = data.get('name')
        row.name_kana = data.get('name_kana')
        row.name_en = data.get('name_en')
        row.category = data.get('category')
        row.type = data.get('type')
        row.place_name = data.get('place_name')
        row.address = data.get('address')
        row.latitude = data.get('latitude')
        row.longitude = data.get('longitude')
        row.url = data.get('url')
        row.note = data.get('note')
        row.duplicate_id = data.get('duplicate_id')
        return row


@dataclass
class ImportPreviewResult:
    """ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼çµæœã‚’è¡¨ã™ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    filename: str
    total_rows: int
    valid_rows: int
    error_rows: int
    duplicate_rows: int
    warning_rows: int
    columns_detected: List[str]
    rows: List[ImportRow]
    detected_encoding: Optional[str] = None  # æ¤œå‡ºã•ã‚ŒãŸã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    
    def to_dict(self) -> Dict[str, Any]:
        """è¾æ›¸å½¢å¼ã«å¤‰æ›"""
        return {
            'filename': self.filename,
            'total_rows': self.total_rows,
            'valid_rows': self.valid_rows,
            'error_rows': self.error_rows,
            'duplicate_rows': self.duplicate_rows,
            'warning_rows': self.warning_rows,
            'columns_detected': self.columns_detected,
            'rows': [row.to_dict() for row in self.rows],
            'detected_encoding': self.detected_encoding,
        }


@dataclass
class ImportExecuteResult:
    """ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Ÿè¡Œçµæœã‚’è¡¨ã™ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    success: bool
    imported_count: int
    skipped_count: int
    error_count: int
    duplicate_count: int
    errors: List[Dict[str, Any]] = field(default_factory=list)
    created_ids: List[int] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        """è¾æ›¸å½¢å¼ã«å¤‰æ›"""
        return {
            'success': self.success,
            'imported_count': self.imported_count,
            'skipped_count': self.skipped_count,
            'error_count': self.error_count,
            'duplicate_count': self.duplicate_count,
            'errors': self.errors,
            'created_ids': self.created_ids,
        }


class CulturalPropertyCSVImporter:
    """æ–‡åŒ–è²¡CSVã‚¤ãƒ³ãƒãƒ¼ã‚¿ãƒ¼"""
    
    # è‡ªæ²»ä½“æ¨™æº–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚«ãƒ©ãƒ åãƒãƒƒãƒ”ãƒ³ã‚°
    # è¤‡æ•°ã®ã‚«ãƒ©ãƒ åã«å¯¾å¿œï¼ˆå„ªå…ˆé †ä½é †ï¼‰
    COLUMN_MAPPING = {
        'name': ['åç§°', 'name'],
        'name_kana': ['åç§°_ã‚«ãƒŠ', 'name_kana', 'ãµã‚ŠãŒãª'],
        'name_en': ['åç§°_è‹±èª', 'name_en', 'è‹±èªå'],
        'category': ['æ–‡åŒ–è²¡åˆ†é¡', 'category', 'ã‚«ãƒ†ã‚´ãƒª'],
        'type': ['ç¨®é¡', 'type', 'ç¨®åˆ¥'],
        'place_name': ['å ´æ‰€åç§°', 'place_name', 'å ´æ‰€å'],
        'address': ['æ‰€åœ¨åœ°_é€£çµè¡¨è¨˜', 'address', 'ä½æ‰€', 'æ‰€åœ¨åœ°'],
        'latitude': ['ç·¯åº¦', 'latitude', 'lat'],
        'longitude': ['çµŒåº¦', 'longitude', 'lng', 'lon'],
        'url': ['URL', 'url', 'å‚è€ƒURL'],
        'note': ['å‚™è€ƒ', 'note', 'èª¬æ˜'],
    }
    
    # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆtypeã¯å¿…é ˆã‹ã‚‰å¤–ã—ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®šï¼‰
    REQUIRED_FIELDS = ['name', 'address', 'latitude', 'longitude']
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    DEFAULT_TYPE = 'ä¸æ˜'
    
    # ç·¯åº¦ã®æœ‰åŠ¹ç¯„å›²ï¼ˆæ—¥æœ¬å›½å†…ï¼‰
    LATITUDE_RANGE = (20.0, 46.0)
    
    # çµŒåº¦ã®æœ‰åŠ¹ç¯„å›²ï¼ˆæ—¥æœ¬å›½å†…ï¼‰
    LONGITUDE_RANGE = (122.0, 154.0)
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ‰åŠ¹æœŸé™ï¼ˆç§’ï¼‰
    SESSION_TIMEOUT = 1800  # 30åˆ†
    
    # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°åˆ¤å®šã®å€™è£œï¼ˆå„ªå…ˆé †ä½é †ï¼‰
    ENCODING_CANDIDATES = [
        'utf-8',
        'utf-8-sig',      # UTF-8 with BOM
        'utf-16',
        'utf-16-le',
        'utf-16-be',
        'cp932',          # Windows Shift-JIS
        'shift-jis',
        'euc-jp',
        'iso-2022-jp',
    ]
    
    def __init__(self, check_duplicates: bool = True):
        """
        åˆæœŸåŒ–
        
        Args:
            check_duplicates: é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚’è¡Œã†ã‹ã©ã†ã‹
        """
        self.check_duplicates = check_duplicates
    
    def detect_encoding(self, file_content: bytes) -> str:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è‡ªå‹•åˆ¤å®š
        
        Args:
            file_content: ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚¤ãƒˆåˆ—
            
        Returns:
            æ¤œå‡ºã•ã‚ŒãŸã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å
        """
        # BOMã«ã‚ˆã‚‹åˆ¤å®š
        if file_content.startswith(b'\xff\xfe'):
            logger.info("ğŸ” BOMæ¤œå‡º: UTF-16 LE")
            return 'utf-16-le'
        if file_content.startswith(b'\xfe\xff'):
            logger.info("ğŸ” BOMæ¤œå‡º: UTF-16 BE")
            return 'utf-16-be'
        if file_content.startswith(b'\xef\xbb\xbf'):
            logger.info("ğŸ” BOMæ¤œå‡º: UTF-8 with BOM")
            return 'utf-8-sig'
        
        # å„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§è©¦ã™
        for encoding in self.ENCODING_CANDIDATES:
            try:
                decoded = file_content.decode(encoding)
                # æ—¥æœ¬èªã®æ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                # ãƒ˜ãƒƒãƒ€ãƒ¼ã«ã€Œåç§°ã€ã€Œç·¯åº¦ã€ã€ŒçµŒåº¦ã€ãªã©ãŒå«ã¾ã‚Œã¦ã„ã‚Œã°æ­£ã—ã„
                if any(keyword in decoded for keyword in ['åç§°', 'ç·¯åº¦', 'çµŒåº¦', 'ä½æ‰€', 'æ‰€åœ¨åœ°']):
                    logger.info(f"ğŸ” ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡º: {encoding}")
                    return encoding
            except (UnicodeDecodeError, UnicodeError):
                continue
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯UTF-8
        logger.warning("âš ï¸ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•åˆ¤å®šå¤±æ•—ã€UTF-8ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¨ã—ã¦ä½¿ç”¨")
        return 'utf-8'
    
    def preview(
        self, 
        file_path: str = None, 
        file_content: bytes = None,
        filename: str = None,
        encoding: str = None  # Noneã®å ´åˆã¯è‡ªå‹•åˆ¤å®š
    ) -> Tuple[ImportPreviewResult, str]:
        """
        CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æã—ã¦ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼çµæœã‚’è¿”ã™
        
        Args:
            file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆCLIç”¨ï¼‰
            file_content: ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ï¼ˆWeb APIç”¨ï¼‰
            filename: ãƒ•ã‚¡ã‚¤ãƒ«å
            encoding: ãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆNoneã®å ´åˆã¯è‡ªå‹•åˆ¤å®šï¼‰
            
        Returns:
            (ImportPreviewResult, session_id): ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼çµæœã¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ID
        """
        logger.info(f"ğŸ“‚ CSVãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼é–‹å§‹: {filename or file_path}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’å–å¾—ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°åˆ¤å®šç”¨ï¼‰
        if file_path and not file_content:
            with open(file_path, 'rb') as f:
                file_content = f.read()
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•åˆ¤å®š
        detected_encoding = encoding
        if not encoding or encoding == 'auto':
            detected_encoding = self.detect_encoding(file_content)
            logger.info(f"ğŸ“‹ è‡ªå‹•æ¤œå‡ºã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {detected_encoding}")
        
        # CSVã‚’èª­ã¿è¾¼ã¿
        rows_data, columns = self._parse_csv(
            file_content=file_content,
            encoding=detected_encoding
        )
        
        # ã‚«ãƒ©ãƒ ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’æ¤œå‡º
        column_map = self._detect_column_mapping(columns)
        logger.info(f"ğŸ“‹ æ¤œå‡ºã•ã‚ŒãŸã‚«ãƒ©ãƒ ãƒãƒƒãƒ”ãƒ³ã‚°: {column_map}")
        
        # å„è¡Œã‚’ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        import_rows: List[ImportRow] = []
        for idx, row_data in enumerate(rows_data, start=1):
            import_row = self._process_row(row_data, idx, column_map)
            import_rows.append(import_row)
        
        # çµ±è¨ˆã‚’è¨ˆç®—
        valid_rows = sum(1 for r in import_rows if r.status == ImportStatus.VALID)
        error_rows = sum(1 for r in import_rows if r.status == ImportStatus.ERROR)
        duplicate_rows = sum(1 for r in import_rows if r.status == ImportStatus.DUPLICATE)
        warning_rows = sum(1 for r in import_rows if r.status == ImportStatus.WARNING)
        
        result = ImportPreviewResult(
            filename=filename or (file_path.split('/')[-1] if file_path else 'unknown.csv'),
            total_rows=len(import_rows),
            valid_rows=valid_rows,
            error_rows=error_rows,
            duplicate_rows=duplicate_rows,
            warning_rows=warning_rows,
            columns_detected=columns,
            rows=import_rows,
            detected_encoding=detected_encoding
        )
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜
        session_id = str(uuid.uuid4())
        self._save_session(session_id, result)
        
        logger.info(f"âœ… CSVãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼å®Œäº†: ç·æ•°={len(import_rows)}, æœ‰åŠ¹={valid_rows}, ã‚¨ãƒ©ãƒ¼={error_rows}, é‡è¤‡={duplicate_rows}")
        
        return result, session_id
    
    def execute(
        self,
        session_id: str = None,
        rows: List[ImportRow] = None,
        created_by: 'User' = None,
        skip_errors: bool = True,
        skip_duplicates: bool = True,
        selected_row_numbers: List[int] = None
    ) -> ImportExecuteResult:
        """
        ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’å®Ÿè¡Œã™ã‚‹
        
        Args:
            session_id: ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼æ™‚ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ID
            rows: ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯¾è±¡ã®è¡Œãƒªã‚¹ãƒˆï¼ˆsession_idãŒãªã„å ´åˆï¼‰
            created_by: ä½œæˆè€…ãƒ¦ãƒ¼ã‚¶ãƒ¼
            skip_errors: ã‚¨ãƒ©ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã‹ã©ã†ã‹
            skip_duplicates: é‡è¤‡è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã‹ã©ã†ã‹
            selected_row_numbers: ç‰¹å®šã®è¡Œç•ªå·ã®ã¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆNoneã®å ´åˆã¯å…¨ã¦ï¼‰
            
        Returns:
            ImportExecuteResult: å®Ÿè¡Œçµæœ
        """
        from cp_api.models import CulturalProperty
        
        logger.info(f"ğŸš€ CSVã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Ÿè¡Œé–‹å§‹")
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰è¡Œãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        if session_id:
            session_data = self._get_session(session_id)
            if not session_data:
                logger.error("âŒ ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return ImportExecuteResult(
                    success=False,
                    imported_count=0,
                    skipped_count=0,
                    error_count=0,
                    duplicate_count=0,
                    errors=[{'message': 'ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒæœŸé™åˆ‡ã‚Œã‹è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'}]
                )
            rows = [ImportRow.from_dict(r) for r in session_data['rows']]
        
        if not rows:
            logger.error("âŒ ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯¾è±¡ã®è¡ŒãŒã‚ã‚Šã¾ã›ã‚“")
            return ImportExecuteResult(
                success=False,
                imported_count=0,
                skipped_count=0,
                error_count=0,
                duplicate_count=0,
                errors=[{'message': 'ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯¾è±¡ã®è¡ŒãŒã‚ã‚Šã¾ã›ã‚“'}]
            )
        
        # ç‰¹å®šè¡Œã®ã¿é¸æŠ
        if selected_row_numbers:
            rows = [r for r in rows if r.row_number in selected_row_numbers]
        
        # ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯¾è±¡ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        rows_to_import = []
        skipped_count = 0
        error_count = 0
        duplicate_count = 0
        
        for row in rows:
            if row.status == ImportStatus.ERROR:
                if skip_errors:
                    skipped_count += 1
                    error_count += 1
                    continue
                else:
                    # ã‚¨ãƒ©ãƒ¼è¡Œã‚’å«ã‚ãªã„å ´åˆã¯å¤±æ•—
                    return ImportExecuteResult(
                        success=False,
                        imported_count=0,
                        skipped_count=skipped_count,
                        error_count=error_count,
                        duplicate_count=duplicate_count,
                        errors=[{'row': row.row_number, 'errors': row.errors}]
                    )
            
            if row.status == ImportStatus.DUPLICATE:
                if skip_duplicates:
                    skipped_count += 1
                    duplicate_count += 1
                    continue
                else:
                    rows_to_import.append(row)
            
            if row.status in [ImportStatus.VALID, ImportStatus.WARNING]:
                rows_to_import.append(row)
        
        # ä¸€æ‹¬ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        created_ids = []
        errors = []
        
        try:
            with transaction.atomic():
                for row in rows_to_import:
                    try:
                        # ã‚¸ã‚ªãƒ¡ãƒˆãƒªã‚’ç”Ÿæˆ
                        geom = Point(row.longitude, row.latitude, srid=6668)
                        
                        # CulturalPropertyã‚’ä½œæˆ
                        cp = CulturalProperty(
                            name=row.name,
                            name_kana=row.name_kana or '',
                            name_en=row.name_en or '',
                            category=row.category or '',
                            type=row.type or self.DEFAULT_TYPE,
                            place_name=row.place_name or '',
                            address=row.address,
                            latitude=row.latitude,
                            longitude=row.longitude,
                            url=row.url or '',
                            note=row.note or '',
                            geom=geom,
                            created_by=created_by
                        )
                        cp.save()
                        created_ids.append(cp.id)
                        
                    except Exception as e:
                        logger.error(f"âŒ è¡Œ{row.row_number}ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—: {e}")
                        errors.append({
                            'row': row.row_number,
                            'name': row.name,
                            'error': str(e)
                        })
                        error_count += 1
        
        except Exception as e:
            logger.error(f"âŒ ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
            return ImportExecuteResult(
                success=False,
                imported_count=0,
                skipped_count=skipped_count,
                error_count=error_count,
                duplicate_count=duplicate_count,
                errors=[{'message': f'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {str(e)}'}]
            )
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤
        if session_id:
            self._delete_session(session_id)
        
        logger.info(f"âœ… CSVã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†: ã‚¤ãƒ³ãƒãƒ¼ãƒˆ={len(created_ids)}, ã‚¹ã‚­ãƒƒãƒ—={skipped_count}")
        
        return ImportExecuteResult(
            success=True,
            imported_count=len(created_ids),
            skipped_count=skipped_count,
            error_count=error_count,
            duplicate_count=duplicate_count,
            errors=errors,
            created_ids=created_ids
        )
    
    def _parse_csv(
        self,
        file_path: str = None,
        file_content: bytes = None,
        encoding: str = 'utf-8'
    ) -> Tuple[List[Dict[str, str]], List[str]]:
        """
        CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§è¡Œãƒ‡ãƒ¼ã‚¿ã¨ã‚«ãƒ©ãƒ åã‚’è¿”ã™
        
        Returns:
            (rows, columns): è¡Œãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆã¨ã‚«ãƒ©ãƒ åã®ãƒªã‚¹ãƒˆ
        """
        if file_path and not file_content:
            with open(file_path, 'rb') as f:
                file_content = f.read()
        
        if not file_content:
            raise ValueError("file_path ã¾ãŸã¯ file_content ãŒå¿…è¦ã§ã™")
        
        # ãƒ‡ã‚³ãƒ¼ãƒ‰
        content = file_content.decode(encoding, errors='replace')
        
        # BOMã‚’é™¤å»ï¼ˆUTF-8-sigã®å ´åˆã¯è‡ªå‹•ã§é™¤å»ã•ã‚Œã‚‹ãŒå¿µã®ãŸã‚ï¼‰
        if content.startswith('\ufeff'):
            content = content[1:]
        
        # æ”¹è¡Œã‚³ãƒ¼ãƒ‰ã‚’çµ±ä¸€
        content = content.replace('\r\n', '\n').replace('\r', '\n')
        
        # CSVã‚’è§£æ
        reader = csv.DictReader(io.StringIO(content))
        columns = reader.fieldnames or []
        rows = list(reader)
        
        return rows, columns
    
    def _detect_column_mapping(self, columns: List[str]) -> Dict[str, str]:
        """
        CSVã®ã‚«ãƒ©ãƒ åã‹ã‚‰å†…éƒ¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’æ¤œå‡º
        
        Returns:
            {å†…éƒ¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å: CSVã‚«ãƒ©ãƒ å} ã®è¾æ›¸
        """
        mapping = {}
        
        for field_name, possible_columns in self.COLUMN_MAPPING.items():
            for col_name in possible_columns:
                if col_name in columns:
                    mapping[field_name] = col_name
                    break
        
        return mapping
    
    def _process_row(
        self,
        row_data: Dict[str, str],
        row_number: int,
        column_map: Dict[str, str]
    ) -> ImportRow:
        """
        1è¡Œã‚’å‡¦ç†ã—ã¦ImportRowã‚’ç”Ÿæˆ
        """
        from cp_api.models import CulturalProperty
        
        import_row = ImportRow(row_number=row_number)
        import_row.raw_data = row_data
        
        # ã‚«ãƒ©ãƒ ãƒãƒƒãƒ”ãƒ³ã‚°ã«å¾“ã£ã¦ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        for field_name, csv_column in column_map.items():
            value = row_data.get(csv_column, '').strip()
            
            # ç©ºæ–‡å­—åˆ—ã¯Noneã«å¤‰æ›
            if value == '':
                value = None
            
            # æ•°å€¤ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å¤‰æ›
            if field_name in ['latitude', 'longitude'] and value:
                try:
                    value = float(value)
                except ValueError:
                    import_row.errors.append(f'{field_name}ãŒæ•°å€¤ã§ã¯ã‚ã‚Šã¾ã›ã‚“: {value}')
                    import_row.status = ImportStatus.ERROR
                    value = None
            
            setattr(import_row, field_name, value)
        
        # typeãŒç©ºã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®šã—ã€è­¦å‘Šã‚’è¿½åŠ 
        if not import_row.type:
            import_row.type = self.DEFAULT_TYPE
            import_row.warnings.append(f'ç¨®é¡ãŒç©ºã®ãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã€Œ{self.DEFAULT_TYPE}ã€ã‚’è¨­å®šã—ã¾ã—ãŸ')
            if import_row.status == ImportStatus.VALID:
                import_row.status = ImportStatus.WARNING
        
        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒã‚§ãƒƒã‚¯
        for field in self.REQUIRED_FIELDS:
            value = getattr(import_row, field, None)
            if value is None or value == '':
                import_row.errors.append(f'{field}ã¯å¿…é ˆé …ç›®ã§ã™')
                import_row.status = ImportStatus.ERROR
        
        # ã‚¨ãƒ©ãƒ¼ãŒãªã‘ã‚Œã°ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ç¶šè¡Œ
        if import_row.status != ImportStatus.ERROR:
            self._validate_row(import_row)
        
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
        if import_row.status != ImportStatus.ERROR and self.check_duplicates:
            if import_row.name and import_row.latitude and import_row.longitude:
                duplicate = self._check_duplicate(
                    import_row.name,
                    import_row.latitude,
                    import_row.longitude
                )
                if duplicate:
                    import_row.status = ImportStatus.DUPLICATE
                    import_row.duplicate_id = duplicate.id
                    import_row.warnings.append(f'æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨é‡è¤‡ã—ã¦ã„ã¾ã™ (ID: {duplicate.id})')
        
        return import_row
    
    def _validate_row(self, row: ImportRow) -> None:
        """
        1è¡Œã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã†ï¼ˆå¿…é ˆãƒã‚§ãƒƒã‚¯ä»¥å¤–ï¼‰
        """
        # ç·¯åº¦ã®ç¯„å›²ãƒã‚§ãƒƒã‚¯
        if row.latitude is not None:
            if not (self.LATITUDE_RANGE[0] <= row.latitude <= self.LATITUDE_RANGE[1]):
                row.errors.append(
                    f'ç·¯åº¦ãŒæ—¥æœ¬å›½å†…ã®ç¯„å›²å¤–ã§ã™: {row.latitude} '
                    f'(æœ‰åŠ¹ç¯„å›²: {self.LATITUDE_RANGE[0]}ã€œ{self.LATITUDE_RANGE[1]})'
                )
                row.status = ImportStatus.ERROR
        
        # çµŒåº¦ã®ç¯„å›²ãƒã‚§ãƒƒã‚¯
        if row.longitude is not None:
            if not (self.LONGITUDE_RANGE[0] <= row.longitude <= self.LONGITUDE_RANGE[1]):
                row.errors.append(
                    f'çµŒåº¦ãŒæ—¥æœ¬å›½å†…ã®ç¯„å›²å¤–ã§ã™: {row.longitude} '
                    f'(æœ‰åŠ¹ç¯„å›²: {self.LONGITUDE_RANGE[0]}ã€œ{self.LONGITUDE_RANGE[1]})'
                )
                row.status = ImportStatus.ERROR
        
        # URLå½¢å¼ãƒã‚§ãƒƒã‚¯ï¼ˆè­¦å‘Šã®ã¿ï¼‰
        if row.url and not (row.url.startswith('http://') or row.url.startswith('https://')):
            row.warnings.append(f'URLã®å½¢å¼ãŒä¸æ­£ã§ã™: {row.url}')
            if row.status == ImportStatus.VALID:
                row.status = ImportStatus.WARNING
        
        # æ–‡å­—åˆ—é•·ãƒã‚§ãƒƒã‚¯
        if row.name and len(row.name) > 254:
            row.errors.append(f'åç§°ãŒé•·ã™ãã¾ã™ (æœ€å¤§254æ–‡å­—)')
            row.status = ImportStatus.ERROR
        
        if row.address and len(row.address) > 254:
            row.errors.append(f'ä½æ‰€ãŒé•·ã™ãã¾ã™ (æœ€å¤§254æ–‡å­—)')
            row.status = ImportStatus.ERROR
    
    def _check_duplicate(
        self,
        name: str,
        latitude: float,
        longitude: float,
        tolerance: float = 0.0001  # ç´„10m
    ) -> Optional['CulturalProperty']:
        """
        é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚’è¡Œã†
        
        åŒä¸€åç§°ã‹ã¤åº§æ¨™ãŒè¿‘ã„ï¼ˆtoleranceã®ç¯„å›²å†…ï¼‰ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œç´¢
        """
        from cp_api.models import CulturalProperty
        
        # åç§°ãŒå®Œå…¨ä¸€è‡´ã—ã€åº§æ¨™ãŒè¿‘ã„ã‚‚ã®ã‚’æ¤œç´¢
        duplicates = CulturalProperty.objects.filter(
            name=name,
            latitude__gte=latitude - tolerance,
            latitude__lte=latitude + tolerance,
            longitude__gte=longitude - tolerance,
            longitude__lte=longitude + tolerance
        )
        
        return duplicates.first()
    
    def _save_session(self, session_id: str, result: ImportPreviewResult) -> None:
        """ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼çµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜"""
        cache.set(
            f"csv_import_session:{session_id}",
            result.to_dict(),
            timeout=self.SESSION_TIMEOUT
        )
        logger.info(f"ğŸ“ ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜: {session_id}")
    
    def _get_session(self, session_id: str) -> Optional[Dict[str, Any]]:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼çµæœã‚’å–å¾—"""
        data = cache.get(f"csv_import_session:{session_id}")
        if data:
            logger.info(f"ğŸ“– ã‚»ãƒƒã‚·ãƒ§ãƒ³å–å¾—: {session_id}")
        else:
            logger.warning(f"âš ï¸ ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {session_id}")
        return data
    
    def _delete_session(self, session_id: str) -> None:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤"""
        cache.delete(f"csv_import_session:{session_id}")
        logger.info(f"ğŸ—‘ï¸ ã‚»ãƒƒã‚·ãƒ§ãƒ³å‰Šé™¤: {session_id}")
